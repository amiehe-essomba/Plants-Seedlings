<h1 align="center"> Plant Seedlings Project </h1>
<img width="1000px" height="300px" src="images/out.png"></img>

**Table des mati√®res :**

- [Introduction](#section-1)
  - [Objectifs du pojet](#sous-section-11)
  - [Pr√©sentation du jeu de donn√©es](#sous-section-12)
- [Analyse exploratoire des donn√©es (EDA)](#section-2)
  - [Visualisation des images de diff√©rentes classes](#sous-section-20)
  - [Pixelisation & R√©partition des classes](#sous-section-21)
  - [Statistiques descriptives sur les donn√©es](#sous-section-22)
  - [Distribution des tailles d'images](#sous-section-23)
  - [Ratio H/L & Canaux RGBA](#sous-section-24)
  - [Histogrammes de Couleurs](#sous-section-25)
- [Pr√©traitement des donn√©es](#section-3)
  - [Redimensionnement des images](#sous-section-31)
  - [Normalisation des valeurs de pixel](#sous-section-32)
  - [Segmentation s√©mantique de l'image](#sous-section-33)
  - [Augmentation de donn√©es](#sous-section-34)
  - [Cr√©ation des ensembles d'entra√Ænement, de validation et de test](#sous-section-35)
- [Construction et entra√Ænement du mod√®le](#section-4)
  - [S√©lection de l'architecture du mod√®le](#sous-section-41)
  - [Mise en place du mod√®le](#sous-section-42)
  - [Configuration de l'entra√Ænement (hyperparam√®tres, fonction de perte, optimiseur)](#sous-section-43)
  - [Entra√Ænement du mod√®le sur les donn√©es](#sous-section-44)
- [√âvaluation du mod√®le](#section-5)
  - [√âvaluation des performances du mod√®le](#sous-section-51)
  - [Mesures des metriques](#sous-section-52)
  - [Matrice de confusion](#sous-section-53)
  - [Courbes ROC (le cas √©ch√©ant)](#sous-section-54)
  - [Analyse des erreurs de classification](#sous-section-54)
- [Am√©lioration du mod√®le](#section-6)
  - [R√©glage des hyperparam√®tres](#sous-section-61)
  - [Utilisation de mod√®les pr√©-entra√Æn√©s (transfer learning)](#sous-section-62)
  - [Entra√Ænement sur des donn√©es suppl√©mentaires (le cas √©ch√©ant)](#sous-section-63)
  - [R√©√©valuation des performances apr√®s les am√©liorations.](#sous-section-64)
- [Visualisation des r√©sultats](#section-7)
  - [Visualisation des pr√©dictions du mod√®le sur de nouvelles images](#sous-section-71)
- [Conclusion](#section-8)
  - [R√©capitulation des r√©sultats et des conclusions](#sous-section-81)
  - [Possibilit√©s d'extensions ou de travaux futurs](#sous-section-82)
- [R√©f√©rences](#section-9)
  
## <a name="section-1"></a>[Introduction](#section-1) 

<p align="left">Les plantes jouent un r√¥le vital dans notre environnement et notre quotidien. Elles fournissent de la nourriture, de l'oxyg√®ne, et contribuent √† la beaut√© de notre monde naturel. Cependant, la classification et la reconnaissance des diff√©rentes esp√®ces de plantes peuvent s'av√©rer √™tre un d√©fi complexe pour les biologistes  et les chercheurs en sciences de l'environnement et naturel. C'est l√† que l'apprentissage automatique, l'apprentissage profond et la vision par ordinateur interviennent pour nous aider √† r√©soudre ce probl√®me si complexe. </p>

Le jeu de donn√©es [Kaggle V2 Plant Seedlings Dataset](https://www.kaggle.com/datasets/vbookshelf/v2-plant-seedlings-dataset) offre une opportunit√© passionnante d'exploration et de comprendre la diversit√© des plantes √† travers une approche informatique bas√© sur du **deep learning**. Compos√© d'une collection d'images de semis de plantes appartenant √† diff√©rentes esp√®ces, ce jeu de donn√©es repr√©sente un d√©fi majeur et int√©ressant pour la classification automatis√©e des plantes. En utilisant des techniques de deep learning avanc√©es(CNN, RCNN etc..) et de traitement d'images (Segmentation d'image), nous pouvons d√©velopper un mod√®le qui sera capable de reconna√Ætre et de classer les plantes en fonction de leurs caract√©ristiques visuelles(nombre de feuilles, croissance, etc..), d√©tecter √©galement les possibles maladies pouvant attaquer ces derni√®res. 

### <a name="sous-section-11"></a>[Ojectifs du projet](#sous-section-11)
<p align="left"> L'objectif principal de ce projet est de cr√©er un mod√®le de deep learning de classification d'images capable de distinguer efficacement avec une tr√®s bonne pr√©cision entre les diff√©rentes esp√®ces de plantes pr√©sentes dans le jeu de donn√©es. Pour ce faire, nous allons explorer les images, pr√©traiter les donn√©es, construire un mod√®le d'apprentissage profond, l'entra√Æner sur un ensemble de donn√©es d'entra√Ænement et √©valuer ses performances sur un ensemble de donn√©es de test et validation. Tout au long de ce projet, nous allons √©galement mettre l'accent sur l'analyse des r√©sultats pour mieux comprendre les performances du mod√®le et identifier les d√©fis sp√©cifiques pos√©s par la classification des plantes et voir comment am√©liorer notre mod√®le de DL. </p>

<p align="left">Ce projet ne se limite pas seulement √† la cr√©ation d'un mod√®le de classification, mais il offre √©galement une opportunit√© d'exploration visuelle des donn√©es, de compr√©hension des techniques d'augmentation de donn√©es et d'analyse des erreurs de classification. En fin de compte, notre objectif secondaire sera d'utiliser les capacit√©s de l'apprentissage profond pour contribuer √† la recherche en botanique et √† la pr√©servation de la biodiversit√© en identifiant automatiquement les esp√®ces de plantes √† partir d'images.</p>

<p align="left">Dans les sections suivantes, nous allons plonger plus profond√©ment dans les d√©tails du jeu de donn√©es, de l'exploration des donn√©es √† la construction de notre mod√®le, en passant par l'√©valuation des performances de ce dernier √† travers plusieurs m√©triques. Nous esp√©rons que ce projet servira de base pour d'autres applications de classification d'images dans le domaine de la botanique et de la biologie.</p>

### <a name="sous-section-12"></a>[Pr√©sentation du jeu de donn√©es](#sous-section-12)

Le jeu de donn√©es [Kaggle V2 Plant Seedlings Dataset](https://www.kaggle.com/datasets/vbookshelf/v2-plant-seedlings-dataset) est un ensemble de donn√©es couramment utilis√© dans le domaine de la vision par ordinateur [1](https://www.kaggle.com/code/allunia/computer-vision-with-seedlings/notebook) et de l'apprentissage automatique pour la classification d'images de plantes [2](https://www.researchgate.net/publication/332677611_An_Improved_Deep_Neural_Network_for_Classification_of_Plant_Seedling_Images). Ce jeu de donn√©es est h√©berg√© sur la plateforme [Kaggle](https://www.kaggle.com/), qui est une communaut√© de data scientists et de chercheurs en science des donn√©es.

Comme d√©crit plus haut dans la section <a name="section-11"></a>[Ojectifs du projet](#sous-section-11), l'objectif principal de ce jeu de donn√©es est de permettre la classification automatique des semis de plantes en fonction de leur esp√®ce. Il s'agit d'une t√¢che de classification multi-classe, o√π chaque image est √©tiquet√©e avec l'esp√®ce de plante correspondante.

Le jeu de donn√©es comprend les √©l√©ments suivants :

> [Images]() : Le jeu de donn√©es contient un ensemble d'images en couleur(**RGB** ou **RGBA**) repr√©sentant des semis de plantes. Chaque image est associ√©e √† une √©tiquette qui indique l'esp√®ce de la plante.

> [Classes]() : Il existe plusieurs classes d'esp√®ces de plantes dans ce jeu de donn√©es. Chaque classe correspond √† une esp√®ce sp√©cifique de plante. Parmi les esp√®ces incluses, on trouve des plantes telles que le ma√Øs, le pissenlit, le ch√©nopode, la renou√©e, la moutarde sauvage, et d'autres *voir tableau ci-dessous*.

> [Taille du Jeu de Donn√©es]() : Le jeu de donn√©es contient un nombre significatif d'images(**5539**), avec plusieurs centaines d'images pour chaque classe. Cependant La taille totale du jeu de donn√©es peut varier en fonction de la version sp√©cifique que vous utilisez. Ici c'est la version **V2 du dataset**.

| __Noms d'esp√®ces__                    | __Nombre de plantes par esp√®ce__ | __RGBA (RGB + canal alpha)__  |__[Total]()__|
|---------------------------------------|----------------------------------|-----------------------------|------|
| __Black-grass__                       | __309__               | __3__|
| __Charlock__                          | __452__               | __0__|
| __Cleavers__                          | __335__               | __0__|
| __Common Chickweed__                  | __713__               | __0__|
| __Common wheat__                      | __253__               | __0__|
| __Fat Hen__                           | __538__               | __0__|
| __Loose Silky-bent__                  | __762__               | __21__|
| __Maize__                             | __257__               | __0__|
| __Scentless Mayweed__                 | __607__               | __0__|
| __Shepherdoco Purse__                 | __274__               | __0__|
| __Small-flowered Cranesbill__         | __576__               | __0__|
| __Sugar beet__                        | __463__               | __0__|
|  __[Total]()__                        | __[5539]()__                     | __[24]()__      |__[12 esp√®ces]()__|

- Valeurs Statistiques

|__Min__        | __Max__     | __Mean__         | __Med__        | __std__       | __Q1__         | __Q3__       | __IQ__        |
|---------------|-------------|------------------|----------------|---------------|----------------|--------------|---------------|
|  __[253]()__  | __[762]()__ | __[462.6]()__    | __[457.5]()__  | __[179.3]()__ | __[300.3]()__  |__[583.8]()__ | __[283.5]()__ |

![logo](/images/boxplots.png)

## <a name="section-2"></a>[Analyse exploratoire des donn√©es (EDA)](#section-2)
### <a name="sous-section-20"></a>[Visualisation des images de diff√©rentes classes](#sous-section-20)
#### Espace colorim√©trique RGB (Red, Green, Blue) :

![logo](/images/rgb.png)
L'espace RGB est bas√© sur les trois canaux de couleur primaires, √† savoir le **rouge (R), le vert (G) et le bleu (B)**. Chaque pixel d'une image est repr√©sent√© par une combinaison de ces trois canaux, ce qui permet de reproduire une large gamme de couleurs.
L'espace RGB est couramment utilis√© dans le traitement d'images et la vision par ordinateur [REF. 6](https://openaccess.thecvf.com/content_eccv_2018_workshops/w31/html/Hesse_Computer_Vision_for_Medical_Infant_Motion_Analysis_State_of_the_ECCVW_2018_paper.html). Il est adapt√© √† de nombreuses t√¢ches, y compris la classification d'images, la d√©tection d'objets, la segmentation s√©mantique d'images   [REF. 5](https://towardsdatascience.com/semantic-segmentation-popular-architectures-dff0a75f39d0), [REF. 4](https://nanonets.com/blog/semantic-image-segmentation-2020/).
Il est intuitif, largement utilis√© et convient bien √† de nombreuses applications de la vision par ordinateur [REF. 6](https://openaccess.thecvf.com/content_eccv_2018_workshops/w31/html/Hesse_Computer_Vision_for_Medical_Infant_Motion_Analysis_State_of_the_ECCVW_2018_paper.html), [REF. 7](https://www.sciencedirect.com/science/article/abs/pii/S0168169919313249).

#### Espace colorim√©trique BGR2-LAB  :
![logo](/images/out.png)
L'espace LAB est un espace colorim√©trique qui est con√ßu pour √™tre perceptuellement uniforme, ce qui signifie que les distances entre les couleurs dans cet espace sont plus coh√©rentes avec la perception humaine de la couleur que dans l'espace RGB.
L'espace LAB est souvent utilis√© pour des t√¢ches o√π la perception de la couleur par l'≈ìil humain est importante. Il est fr√©quemment utilis√© en imagerie m√©dicale, en conception graphique et en analyse de la couleur.
Il est adapt√© √† des t√¢ches o√π la pr√©cision de la correspondance des couleurs est cruciale. L'espace LAB est ind√©pendant du p√©riph√©rique, ce qui signifie qu'il est moins sensible aux variations de couleur dues aux diff√©rents √©crans et appareils.

L'utilisation de deux espaces colorim√©triques (RGB et LAB) peut √™tre int√©ressante pour explorer diff√©rentes approches de pr√©traitement des images et √©valuer comment ces espaces affectent les performances de votre mod√®le.  Nous allons voir comment l'utilisation de l'espace BGR2-LAB 
peut faciliter la segmentation s√©mantique de l'image, et d√©bruit√© une image avec une √©fficacit√© redoutable.

### <a name="sous-section-21"></a>[Pixelisations & R√©partition des classes](#sous-section-21)
![logo](/images/hist_bar.png)

Les deux graphiques ci-dessus pr√©sentent la r√©partition du nombre de pixels et du nombre de plantes par esp√®ce. De ces graphiques, deux observations importantes se d√©gagent :

> √âtant donn√© que la taille d'un pixel est d√©finie comme $pixel = (largeur * hauteur)$, on constate une concentration des valeurs autour de l'intervalle [0.1, 0.4] mega pixels, indiquant une certaine h√©t√©rog√©n√©it√© dans les donn√©es, ainsi que la pr√©sence de quelques valeurs aberrantes donc la plus grande valeur est de 3.6 Mega Pixels. 

> On remarque √©galement que le nombre d'exemplaires varie d'une esp√®ce √† l'autre, ce qui se refl√®te par une disparit√© significative dans l'histogramme en barres. Cette disparit√© d√©montre que les donn√©es ne sont pas distribu√©es uniform√©ment dans ce dataset. En effet, nous avons des classes dominantes poss√©dant 2 √† 3 fois plus de valeurs que les classes minoritaires. Cette disparit√© peut poser des d√©fis importants lors de la construction et de l'entra√Ænement d'un mod√®le de deep learning, car il peut avoir tendance √† √™tre biais√© en faveur des classes majoritaires, n√©gligeant ainsi les classes minoritaires. Dans le cas des donn√©es tabulaires, des techniques d'√©quilibrage des donn√©es, telles que la sur-√©chantillonnage (oversampling) des classes minoritaires ou la sous-√©chantillonnage (undersampling) des classes majoritaires, peuvent √™tre n√©cessaires pour garantir que le mod√®le apprend efficacement √† discriminer toutes les classes avec une pr√©cision √©quilibr√©e. Dans ce cas pr√©cis il est √©vident que le data augmentation sera au centre de ce processus pour obtenir un √©quilibre entre les classes.

Afin d'atteindre l'objectif d'une taille d'√©chantillonnage uniforme, la normalisation des donn√©es est cruciale. La data augmentation joue un r√¥le fondamental dans ce processus. En utilisant des techniques de data augmentation telles que la rotation, le redimensionnement et le recadrage, changement l√©ger de pixels, nous pouvons g√©n√©rer des versions modifi√©es des images existantes, les rendant coh√©rentes en termes de taille. Cela permet non seulement d'am√©liorer la qualit√© et la diversit√© du jeu de donn√©es, mais aussi d'augmenter la robustesse du mod√®le de **deep learning** en l'entra√Ænant sur une vari√©t√© d'angles et de perspectives des donn√©es. Ainsi, un mod√®le form√© sur un jeu de donn√©es normalis√© et augment√© est plus susceptible de g√©n√©raliser correctement lorsqu'il est confront√© √† de nouvelles donn√©es.

### <a name="sous-section-23"></a>[Distribution des tailles d'images](#sous-section-23)
![logo](/images/hist_hist.png)

Les deux premiers graphiques ci-dessus pr√©sentent la r√©partition des hauteurs et des largeurs des images de diff√©rentes esp√®ces de plantes. De ces graphiques, trois observations importantes √©mergent :

> Hauteur et largeur similaires : √Ä une exception pr√®s, la hauteur et la largeur pr√©sentent des variations similaires, sugg√©rant ainsi que la largeur est approximativement √©gale √† la hauteur pour la plupart des images du dataset. En notant n_H et n_W comme les hauteurs et largeurs de chaque image, cela peut √™tre exprim√© comme (m, n_H, n_W, n_C) ‚âà (m, n_H, n_H, n_C).

> Variabilit√© des dimensions : Les images du dataset ne pr√©sentent pas toutes les m√™mes dimensions. Par cons√©quent, il est n√©cessaire de les redimensionner √† la m√™me taille avant d'entreprendre toute op√©ration de mod√©lisation.

> Choix de la taille de redimensionnement : Les donn√©es montrent une concentration autour de [90, 250], avec des pics plus marqu√©s. C'est √† ce stade qu'il est crucial de d√©cider de la taille de redimensionnement. Dans ce contexte, une taille de redimensionnement de (160, 160) a √©t√© choisie. Cependant, plusieurs versions du dataset seront g√©n√©r√©es avec des tailles de redimensionnement diff√©rentes afin d'obtenir des donn√©es de qualit√© ainsi qu'une vari√©t√© de donn√©es de qualit√©.

La diversit√© des tailles d'images peut poser des d√©fis pour le traitement num√©rique, car de nombreuses op√©rations de mod√©lisation n√©cessitent des images de taille uniforme. Ainsi, le redimensionnement des images pour les adapter √† une taille commune, telle que (160, 160, 3), est essentiel pour garantir la coh√©rence dans l'ensemble du dataset.

Cependant, il est important de noter que diff√©rentes tailles de redimensionnement peuvent √™tre utilis√©es en fonction des besoins sp√©cifiques de l'application. Par exemple, une taille plus petite peut √™tre privil√©gi√©e pour une meilleure vitesse de traitement, tandis qu'une taille plus grande peut √™tre pr√©f√©rable pour une qualit√© visuelle sup√©rieure.

En fin de compte, le choix de la taille de redimensionnement d√©pend des compromis entre la qualit√©, la vitesse et les ressources disponibles. Il est √©galement judicieux de g√©n√©rer plusieurs versions du dataset avec diff√©rentes tailles de redimensionnement pour r√©pondre √† diverses exigences d'analyse et d'apprentissage profond.


### <a name="sous-section-24"></a>[Ratio H/L & Canaux RGBA](#sous-section-24)
![logo](/images/hist_pie.png)

Les deux graphiques ci-dessus pr√©sentent la r√©partition du rapport *Largeur/Hauteur* et du nombre de canaux **RGBA** pr√©sents dans chaque esp√®ce de plantes ([voir √©galement ces valeurs dans le tableau plus haut]()). De ces graphiques, deux observations importantes se d√©gagent :

> Le premier graphique confirme ce qui a √©t√© mentionn√© pr√©c√©demment, √† savoir que la grande majorit√© des images ont une relation *n_W ‚âà n_H*. Cependant, ce n'est pas le cas pour toutes les images et esp√®ces. C'est pourquoi il est imp√©ratif de redimensionner toutes les images √† une taille fixe o√π *n_H = n_W* pour garantir des dimensions uniformes pour toutes les images, quelle que soit l'esp√®ce de plantes.

> Nous constatons que les images de chaque esp√®ce ne sont pas toutes au format **RGB**. Certaines sont au format **RGBA=RGB+alpha**, ce qui pose un probl√®me pour assurer une coh√©rence dans la repr√©sentation des couleurs. Il est donc n√©cessaire de retirer le canal *alpha* pour passer de **RGBA** √† **RGB**.

En ce qui concerne la premi√®re observation, il est essentiel de garantir que toutes les images, ind√©pendamment de leur esp√®ce, aient des dimensions √©gales. Cela permet une manipulation uniforme lors du traitement num√©rique et de la formation de mod√®les d'apprentissage profond.

Quant √† la deuxi√®me observation, la pr√©sence des canaux *alpha* dans certaines images peut cr√©er une incoh√©rence dans la repr√©sentation des couleurs, car ces canaux *alpha* correspondent √† la transparence des pixels, ce qui n'est g√©n√©ralement pas n√©cessaire dans le contexte du traitement d'images de plantes. Par cons√©quent, il est recommand√© de retirer ce canal pour toutes les images, passant ainsi au bon format **RGB**.

En effectuant ces transformations pr√©liminaires sur les donn√©es, nous nous assurons que les images sont pr√™tes √† √™tre utilis√©es de mani√®re efficace dans la cr√©ation d'un mod√®le de deep learning. Cela garantit une repr√©sentation coh√©rente des couleurs et des dimensions, favorisant ainsi une analyse homog√®ne des donn√©es.

### <a name="sous-section-25"></a>[Histogrammes de Couleurs](#sous-section-25)
![logo](/images/hist_color.png)

L'histogramme de couleurs est un outil tr√®s puissant permettant de visualiser l'intensit√© en fonction de la pix√©lisation des couleurs. Il est g√©n√©ralement utilis√© dans le traitement d'image pour manipuler les trois canaux (r, g, b) en fragmentant l'image. Cela sert d'abord √† s√©lectionner le canal le mieux adapt√© pour la s√©paration des couleurs, puis √† choisir un domaine de valeurs ou √† appliquer un filtre de fragmentation.

Sur la figure ci-dessus, sont repr√©sent√©es 12 histogrammes diff√©rents regroup√©s en trois classes dans l'espace colom√©trique **LAB** repr√©sentant quatre esp√®ces de plantes diff√©rentes. L'espace colom√©trique LAB est un espace qui tente de repr√©senter de mani√®re plus pr√©cise la perception humaine des couleurs par rapport au **RGB**. Pour demontrer sa puissance toutes les images sont pass√©es du format **RGB** standard au **RGB-LAB** tout en conservant 3 canaux (r, g, b). Il se compose de trois composantes principales:

- *L* (Luminance) : repr√©sente la luminosit√© ou la clart√© de la couleur.
- *A* : repr√©sente la gamme de couleurs de vert √† magenta.
- *B* : repr√©sente la gamme de couleurs de bleu √† jaune.

Il est important de noter que pour effectuer la segmentation des couleurs, plusieurs pics doivent appara√Ætre dans ces histogrammes pour mieux cat√©goriser les couleurs. Ainsi, plus un histogramme pr√©sente de pics, plus il est facile de distinguer les couleurs apparentes et plus il est facile de cr√©er la fragmentation. En observant davantage les figures ci-dessus, on peut se rendre compte que les histogrammes obtenus dans l'espace **RGB-LAB**, contrairement √† l'espace **RGB**, peuvent inclure des valeurs n√©gatives, ce qui indique que le format de couleurs n'est pas RGB, mais plut√¥t **RGB-LAB**. Cela est particuli√®rement important pour comprendre les propri√©t√©s des couleurs dans ce contexte.
Notre segmentation sur l'image originale (RGB) a √©t√© effectu√©e sur ce canal (canal 1), comme vous le verrez par la suite.

Nous verrons dans la section [Segmentation s√©mantique de l'image](#sous-section-33) comment choisir :
- le canal optimal pour cr√©er un filtre ultra puissant permettant de d√©bruiter une image en profondeur.
- le choix du domaine de valeurs pour un nettoyage en profondeur 

## <a name="section-3">[Pr√©traitement des donn√©es](#section-3)
###  <a name="sous-section-31">[Redimensionnement des images](#sous-section-31)
Toutes les images on √©t√© redimention√©es au format adapt√© √† savoir (160 x 160 )  soit 25600 pixels par image. ce pendant d'autres formats seront produits pour enrichir le la base donn√©e et tester plusieurs taille diff√©rentes pour notre mod√®le de deep learning.
Cette uniformisation des dimensions garantit une coh√©rence dans le traitement num√©rique et facilite la cr√©ation de mod√®les d'apprentissage profond.


###  <a name="sous-section-32">[Normalisation des valeurs de pixel](#sous-section-32)
Afin d'√©viter l'explosion du gradient lors de l'optimisation du mod√®le de deep learning, il est imp√©ratif de normaliser les donn√©es. Dans le cadre de ce dataset, cette normalisation a √©t√© effectu√©e en divisant toutes les valeurs de pixel par **255.**.

La normalisation des donn√©es est une √©tape fondamentale pour mettre toutes les caract√©ristiques √† la m√™me √©chelle, ce qui facilite la convergence de l'algorithme d'optimisation (**Descente du gradient**) lors de l'entra√Ænement du mod√®le. En divisant les valeurs par 255, nous ramenons les pixels √† une √©chelle de -1 √† 1 **(RGR2-LAB)** or 0 et 1 **(RGB)**, ce qui est particuli√®rement important dans le cas des images en couleurs o√π chaque canal de couleur (rouge, vert, bleu) varie de 0 √† 255.

Cette √©tape de normalisation garantit que le mod√®le peut apprendre efficacement √† partir des donn√©es sans √™tre perturb√© par des valeurs d'√©chelle diff√©rentes. Elle contribue √† stabiliser le processus d'entra√Ænement, minimiser la variance et √† am√©liorer la performance globale du mod√®le de deep learning en permettant une convergence plus rapide et plus stable lors de l'optimisation.

###  <a name="sous-section-33">[Segmentation s√©mantique de l'image](#sous-section-33)
![logo](/images/all_process.png)

Sur cette figure, les diff√©rentes √©tapes de la segmentation d'image sont d√©taill√©es :

> Le premier panneau contient uniquement les images originales, qui peuvent pr√©senter un arri√®re-plan plus ou moins bruit√©. L'objectif ici est de d√©bruiter ces images pour obtenir des images avec un fond neutre (noir ou blanc). Cela permet √† notre mod√®le de deep learning de se concentrer sur l'essentiel, c'est-√†-dire la reconnaissance des diff√©rentes classes de plantes. Pour y parvenir, un premier filtre a √©t√© cr√©√© en se basant sur les histogrammes de couleurs pr√©sent√©s pr√©c√©demment. Le choix du canal s'est port√© sur le canal 1. Vous pourriez vous demander pourquoi le canal 1 et non le canal 0 ou le canal 2 ? La raison est simple : les images sont constitu√©es principalement de deux tons, soit du vert (repr√©sentant les plantes), soit du marron (correspondant au bruit de fond). Ainsi, pour extraire la plante, il est essentiel de se concentrer sur le vrai canal, c'est-√†-dire le vert (canal 1). Cependant, conna√Ætre le canal ne suffit parfois pas, il est √©galement n√©cessaire de savoir appliquer la segmentation en d√©finissant un domaine de valeurs appropri√©.

> Le deuxi√®me panneau contient la projection en 2D (canal 1) des images au format RGB-LAB. Ces couleurs varient du vert au magenta, ce qui signifie que les premiers bins contiennent la couleur verte (correspondant au premier pic de l'histogramme). Pour d√©terminer la valeur maximale du domaine de couleurs, il est n√©cessaire de rechercher le minimum absolu apr√®s le premier maximum local. Cependant, cette valeur minimale n'est pas unique et d√©pend fortement de la plante en question. Pour approximer cette valeur, dans ce projet, la valeur maximale a √©t√© d√©finie comme `bin_max = -25`. La valeur minimale peut correspondre √† n'importe quelle valeur situ√©e avant le premier maximum local. Dans ce contexte, la valeur minimale a √©t√© d√©finie comme `bin_min = -200`. Ainsi, le domaine de valeurs devient : `D = [-200, -25]`. De plus, un rayon `r = 3.8` a √©t√© utilis√© pour les op√©rations d'√©rosion et de dilatation.

> Le troisi√®me panneau illustre les r√©sultats obtenus en appliquant le domaine D et le rayon r sur les diff√©rentes images. Ce r√©sultat est appel√© la segmentation s√©mantique de l'image [REF. 4](https://nanonets.com/blog/semantic-image-segmentation-2020/), une √©tape cruciale qui permet d'isoler les objets non pertinents et de capturer les objets d'int√©r√™t. La segmentation s√©mantique vise √† cr√©er une image o√π chaque pixel est associ√© √† une classe sp√©cifique, d√©terminant ainsi quelles parties de l'image appartiennent √† des objets d'int√©r√™t et quelles parties sont consid√©r√©es comme des objets inutiles ou d'arri√®re-plan. Cette √©tape est essentielle pour pr√©parer les donn√©es avant de les soumettre √† un mod√®le de deep learning, car elle permet au mod√®le de se concentrer sur les √©l√©ments importants de l'image, en l'occurrence, la reconnaissance des diff√©rentes classes de plantes. Le domaine D et le rayon r, que nous avons d√©finis pr√©c√©demment, jouent un r√¥le crucial dans cette op√©ration de segmentation s√©mantique en permettant de d√©limiter avec pr√©cision les objets d'int√©r√™t dans l'image et d'√©liminer le bruit ou les informations non pertinentes. Le r√©sultat obtenu dans ce panneau constitue la base pour obtenir la premi√®re image final sur fond noir 

> Le quatri√®me panneau repr√©sente une combinaison entre l'image segment√©e et l'image originale. Le fond noir de cette image r√©sulte du fait que, lors de la cr√©ation de l'image interm√©diaire, tout ce qui est consid√©r√© comme des objets non pertinents est converti en valeur 0. Cette √©tape de combinaison entre l'image segment√©e et l'image originale cr√©e une image interpr√©table et nette. 

> Le dernier panneau est obtenu en rempla√ßant simplement le fond noir par du blanc. Pour y parvenir, il est n√©cessaire de d√©limiter le domaine du noir, qui est g√©n√©ralement compris entre 0 et 30 en termes de valeurs de pixels.En effectuant cette op√©ration, le fond noir pr√©sent dans l'image pr√©c√©dente est remplac√© par du blanc, cr√©ant ainsi un contraste clair entre les objets d'int√©r√™t et l'arri√®re-plan. Cette √©tape permet de rendre les objets d'int√©r√™t encore plus visibles et de faciliter la distinction entre eux et l'arri√®re-plan. Le dernier panneau ainsi obtenu repr√©sente une image nette et bien d√©finie, o√π les objets d'int√©r√™t ressortent clairement sur un fond blanc. Ces images (fonds noirs et blancs) servira de donn√©e d'entra√Ænement pour notre mod√®le de deep learning, lui permettant de mieux comprendre et reconna√Ætre les diff√©rentes classes de plantes.

###  <a name="sous-section-34">[Augmentation de donn√©es](#sous-section-34)
![logo](/images/img_aug.png)

Dans ce projet, deux m√©thodes diff√©rentes avec TensorFlow ont √©t√© exp√©riment√©es pour augmenter les donn√©es :

> La premi√®re m√©thode vise √† r√©soudre le probl√®me de disparit√© des classes dans le dataset en √©quilibrant les classes. En d'autres termes, toutes les classes ont le m√™me nombre d'√©chantillons d'images, soit environ `5000` images par classe, ce qui repr√©sente au total `60000` images pour l'ensemble du dataset.

> La deuxi√®me approche consiste √† charger les donn√©es progressivement dans le mod√®le en g√©n√©rant un flux de donn√©es al√©atoire.

Les modifications apport√©es √† chaque image dans le cadre de ces deux approches comprennent :
- La rotation (360¬∞)
- Le recadrage
- Le zoom
- De l√©g√®res modifications de couleur
- La normalisation

La figure ci-dessus illustre la premi√®re m√©thode, o√π √† partir d'une image originale, 18 images diff√©rentes ont √©t√© g√©n√©r√©es.

###  <a name="sous-section-35">[Cr√©ation des ensembles d'entra√Ænement, de validation et de test](#sous-section-35)
Pour √©viter tout surapprentissage du mod√®le sur les donn√©es d'entra√Ænement et garantir une √©valuation pr√©cise de sa performance, l'ensemble de donn√©es a √©t√© divis√© de la mani√®re suivante :

- 20 % du dataset ont √©t√© r√©serv√©s pour l'ensemble de test et l'ensemble de validation, avec 10 % de donn√©es dans chaque ensemble.
- Les 80 % restants du dataset ont √©t√© utilis√©s pour constituer l'ensemble d'entra√Ænement.

Cette division permet de s'assurer que le mod√®le est entra√Æn√© sur une grande partie des donn√©es tout en conservant des ensembles de test et de validation ind√©pendants pour √©valuer sa capacit√© √† g√©n√©raliser sur de nouvelles donn√©es. Elle contribue √† minimiser les risques de surapprentissage, de biaisage du mod√®le et √† obtenir une √©valuation fiable de la performance du mod√®le.


## <a name="section-4">[Construction et entra√Ænement du mod√®le](#section-4)
###  <a name="sous-section-41">[S√©lection de l'architecture du mod√®le](#sous-section-41)

Plusieurs mod√®les sont disponibles pour mod√©liser cet ensemble de donn√©es, notamment les r√©seaux de neurones convolutionnels (CNN), les r√©seaux de neurones profonds (DNN) et les perceptrons multicouches (MLP), √©galement les vision-Transformers(ViT). Cependant, dans le cas des MLP et des DNN, l'architecture du mod√®le se compose uniquement de couches enti√®rement connect√©es (FC) avec un certain nombre de neurones par couche. Bien que ces architectures soient g√©n√©ralement utilis√©es pour des probl√®mes de classification, elles ne sont pas id√©ales pour ce cas d'√©tude. Ces mod√®les peuvent facilement √™tre biais√©s et tomber dans une phase de surapprentissage importante.

En revanche, l'architecture des CNN et celle de ViT sont con√ßuent de mani√®re √† capturer en premier lieu les caract√©ristiques les plus importantes des images, telles que la forme, la couleur, la texture, etc., tout en effectuant la r√©duction de dimensions, ce qui est crucial pour la dur√©e de simulation. C'est ce qui distingue les CNN/ViT et les rendent tr√®s efficaces dans le domaine de l'imagerie, de la vision par ordinateur, du transfert de style neuronal ou m√™me de la d√©tection et reconnaissance de visages. La puissance des r√©seaux de neurones convolutionnels sera exploit√©e ici pour obtenir de bonnes performances dans la classification des esp√®ces de plantes.

> plus de d√©tails sur l'architecture du mod√®le CNN utilis√© [ici](https://github.com/amiehe-essomba/plants_project/blob/computer-vision/CnnNet.md)

###  <a name="sous-section-42">[Mise en place du mod√®le](#sous-section-42)
Plus de d√©tails sur la mise en place du mod√®lel [ici](https://github.com/amiehe-essomba/plants_project/blob/computer-vision/CnnNet.md)

###  <a name="sous-section-43">[Configuration de l'entra√Ænement (hyperparam√®tres, fonction de perte, optimiseur)](#sous-section-43)

1. __```function perte pour √©valuer les performances de la DG```__
  * categorical_crossentropy

2. __```Type d'optimiseur pour la descente de gradient (DG)```__
  * Adam(learnig_rate=1e-3, beta_1=0.9, beta_2=0.999)

3. __```metriques utilis√©e pour √©valuer les performances du mod√®le```__ 
  * accuracy
  * recall
  * matrice de confusion 

4. __```Callbacks```__

  Voir les d√©tails de callbacks [ici](https://github.com/amiehe-essomba/plants_project/blob/computer-vision/CnnNet.md)

> Enregistrement du meilleurs mod√®le 
> Enreistrement des mod√®les √† chaque √©poque
> Configuration du EarlyStopping 
> Configuration du ReduceLROnPlateau 

###  <a name="sous-section-44">[Entra√Ænement du mod√®le sur les donn√©es](#sous-section-44)

Le mod√®le de CNN qui a √©t√© entrain√© sur :
> 100 √©poques 
> learning_rate de 1e-3 et qui a √©t√© modifi√© evec le ReduceRLOnPlateau jusqu'a la veleur de 1e-5
> un batch_size de 128 pour le training et 64 pour le testing 
> dur√©e total 24h


## <a name="section-4">[√âvaluation du mod√®le](#section-5)
Une fois que la phase de pr√©traitement achev√©, une normalisation des donn√©es par 255. pixels effectu√©e et les hyper param√®tres d√©finis
une prmi√®re mod√©lisation √† √©t√© efectu√©e sur des images avec un fond blanc, suivie d'une seconde mod√©lisation avec un fond noir et affin une mod√©lisation avec un fond normal.

### <a name="section-51">[√âvaluation des performances du mod√®le](#sous-section-51)
![logo](/images/accuracy.png)
![logo](/images/accuracy_b.png)

## Acknowledgement :



## Authors : 
* __**```Dr. Ir√©n√© A. Essomba```**__
[citation](https://vision.eng.au.dk/plant-seedlings-dataset/)

## ü§ù Support 
Give a ‚≠ê if you like this project!

## License 
Copyrihght ¬© 2023 __**Ir√©n√© A. Essomba**__

This project is licensed under [MIT License](https://github.com/amiehe-essomba/plants_project/blob/computer-vision/LICENSE)

